{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from Data import AnnaDataset, InvertAnna, IntcodeAnna\n",
    "from Models import BRNN\n",
    "from Utils.HelperFunctions import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "import gc\n",
    "import os\n",
    "%matplotlib inline  \n",
    "# use gpu when possible\n",
    "mydevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "BATCH_SIZE = 512\n",
    "N_STEPS = 10\n",
    "N_HIDDEN = 512\n",
    "N_PART = 2\n",
    "K = 5 \n",
    "N_EPOCHS = 101\n",
    "learning_rates = np.asarray([1e-5])\n",
    "lr_ratio =  np.asarray([1e-2]) # relative speed of hyper to base parameter optimization\n",
    "clip = 5\n",
    "N_REPS = 2\n",
    "\n",
    "# data set split (consider moving into the AnaDataset instead)\n",
    "dataset = AnnaDataset(N_STEPS)\n",
    "n = len(dataset)  # how many total elements you have\n",
    "n_test = int( n * .1 )  # number of test/val elements\n",
    "n_train = n - 2 * n_test\n",
    "idx = list(range(n))  # indices to all elements\n",
    "np.random.shuffle(idx)  # in-place shuffle the indices to facilitate random splitting\n",
    "train_idx = idx[:n_train]\n",
    "val_idx = idx[n_train:(n_train + n_test)]\n",
    "test_idx = idx[(n_train + n_test):]\n",
    "\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_set = torch.utils.data.Subset(dataset, val_idx)\n",
    "test_set = torch.utils.data.Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(val_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=4)\n",
    "\n",
    "N_INPUTS = len(dataset.categories)\n",
    "N_OUTPUTS = N_INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a30534f26284cec9cb671a6de96d8ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb247053aee147c0abe4cc7dc88b3ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=101), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is a model trained with no regularization on any blocks\n",
    "train0_loss = np.zeros((N_EPOCHS,N_REPS))\n",
    "train0_acc = np.zeros((N_EPOCHS,N_REPS))\n",
    "test0_loss = np.zeros((N_EPOCHS,N_REPS))\n",
    "test0_acc = np.zeros((N_EPOCHS,N_REPS))\n",
    "\n",
    "dummy_model = BRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,N_PART,K,mydevice)\n",
    "lambda0_hist = np.zeros((len(dummy_model.regularize.weight.data.flatten()),N_EPOCHS+1,N_REPS))\n",
    "del(dummy_model)\n",
    "\n",
    "model0 = [None]*N_REPS\n",
    "best_model0 = [None]\n",
    "best_test0_acc = 0\n",
    "for rep in tnrange(N_REPS):\n",
    "    model0[rep] = BRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,N_PART,K,mydevice)\n",
    "    optimizer_base = torch.optim.RMSprop(model0[rep].parameters(), lr=learning_rates[0], momentum=0.9)\n",
    "    criterion_base = nn.CrossEntropyLoss()\n",
    "    lambda0_hist[:,0,rep] = model0[rep].regularize.weight.data.flatten().cpu().numpy()\n",
    "    for epoch in tnrange(N_EPOCHS):\n",
    "        running_train_loss = 0\n",
    "        running_train_acc = 0\n",
    "        running_test_loss = 0\n",
    "        running_test_acc = 0\n",
    "        # train\n",
    "        model0[rep].train()\n",
    "        for i, (train_data, val_data) in enumerate(zip(train_loader,cycle(validation_loader))):\n",
    "            x_train, y_train = train_data\n",
    "            x_val, y_val = val_data\n",
    "            # calculate loss on training dataset\n",
    "            model0[rep].zero_grad()\n",
    "            x_train, y_train = x_train.to(mydevice), y_train.to(mydevice)\n",
    "            y_train_pred, hidden = model0[rep](x_train)\n",
    "            loss_base = criterion_base(y_train_pred[-1,:,:],y_train)\n",
    "            # record performance on training dataset\n",
    "            running_train_loss+=loss_base.item()\n",
    "            running_train_acc+=get_accuracy(y_train_pred[-1,:,:], y_train)\n",
    "            # step the base optimizer\n",
    "            loss_base.backward()\n",
    "            nn.utils.clip_grad_norm(model0[rep].parameters(), clip) # clip gradient to prevent explosion\n",
    "            optimizer_base.step()\n",
    "            \n",
    "        # update history \n",
    "        lambda0_hist[:,epoch+1,rep] = model0[rep].regularize.weight.data.flatten().cpu().numpy()\n",
    "        train0_loss[epoch,rep] = running_train_loss/(i+1)\n",
    "        train0_acc[epoch,rep] = running_train_acc/(i+1)\n",
    "        \n",
    "        # test\n",
    "        model0[rep].eval()\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            x_test, y_test = test_data\n",
    "            x_test, y_test = x_test.to(mydevice), y_test.to(mydevice)\n",
    "            y_test_pred, hidden = model0[rep](x_test)\n",
    "            loss = criterion_base(y_test_pred[-1,:,:],y_test)\n",
    "            # record performance on validation dataset\n",
    "            running_test_loss+=loss.item()\n",
    "            running_test_acc+=get_accuracy(y_test_pred[-1,:,:], y_test)\n",
    "            \n",
    "        # update history \n",
    "        test0_loss[epoch,rep] = running_test_loss/(i+1)\n",
    "        test0_acc[epoch,rep] = running_test_acc/(i+1)\n",
    "         \n",
    "    if test0_acc[-1,rep]>best_test0_acc:\n",
    "        best_model0 = model0[rep]\n",
    "        best_test0_acc = test0_acc[-1,rep]\n",
    "    model0[rep] = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = np.zeros((N_EPOCHS,N_REPS))\n",
    "train_acc = np.zeros((N_EPOCHS,N_REPS))\n",
    "test_loss = np.zeros((N_EPOCHS,N_REPS))\n",
    "test_acc = np.zeros((N_EPOCHS,N_REPS))\n",
    "\n",
    "dummy_model = BRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,N_PART,K,mydevice)\n",
    "lambda_hist = np.zeros((len(dummy_model.regularize.weight.data.flatten()),N_EPOCHS+1,N_REPS))\n",
    "del(dummy_model)\n",
    "\n",
    "model = [None]*N_REPS\n",
    "best_model = [None]\n",
    "best_test_acc = 0\n",
    "for rep in tnrange(N_REPS):\n",
    "    model_path = r'D:\\chongguo\\git\\MLClubModelZoo\\PRNN\\final_project\\History\\model_lr_1en5_lr_ratio_1en1_rep{}.pt'.format(rep)\n",
    "    model[rep] = BRNN(N_INPUTS,N_HIDDEN,N_OUTPUTS,N_PART,K,mydevice)\n",
    "    optimizer_base = torch.optim.RMSprop(model[rep].parameters(), lr=learning_rates[0], momentum=0.9)\n",
    "    criterion_base = nn.CrossEntropyLoss()\n",
    "    optimizer_super = torch.optim.RMSprop(model[rep].parameters(), lr=lr_ratio[0]*learning_rates[0], momentum=0.9)\n",
    "    criterion_super = nn.CrossEntropyLoss()\n",
    "    lambda_hist[:,0,rep] = model[rep].regularize.weight.data.flatten().cpu().numpy()\n",
    "    for epoch in tnrange(N_EPOCHS):\n",
    "        running_train_loss = 0\n",
    "        running_train_acc = 0\n",
    "        running_test_loss = 0\n",
    "        running_test_acc = 0\n",
    "        # train\n",
    "        model[rep].train()\n",
    "        for i, (train_data, val_data) in enumerate(zip(train_loader,cycle(validation_loader))):\n",
    "            x_train, y_train = train_data\n",
    "            x_val, y_val = val_data\n",
    "            # calculate loss on training dataset\n",
    "            model[rep].zero_grad()\n",
    "            x_train, y_train = x_train.to(mydevice), y_train.to(mydevice)\n",
    "            y_train_pred, hidden = model[rep](x_train)\n",
    "            loss_base = criterion_base(y_train_pred[-1,:,:],y_train)\n",
    "            # record performance on training dataset\n",
    "            running_train_loss+=loss_base.item()\n",
    "            running_train_acc+=get_accuracy(y_train_pred[-1,:,:], y_train)\n",
    "            # step the base optimizer\n",
    "            loss_base = loss_base + model[rep].block_norm()/BATCH_SIZE\n",
    "            loss_base.backward()\n",
    "            model[rep].regularize.weight.grad.data.zero_() # zero the gradient of the regularizer\n",
    "            nn.utils.clip_grad_norm(model[rep].parameters(), clip) # clip gradient to prevent explosion\n",
    "            optimizer_base.step()\n",
    "            \n",
    "            # caculate loss on validation dataset\n",
    "            model[rep].zero_grad()\n",
    "            x_val, y_val = x_val.to(mydevice), y_val.to(mydevice)\n",
    "            y_val_pred, hidden = model[rep](x_val)\n",
    "            loss_super = criterion_super(y_val_pred[-1,:,:],y_val)\n",
    "            # step the hyperparameter optimizer\n",
    "            loss_super = loss_super+model[rep].zero_regularize_grad()\n",
    "            loss_super.backward()\n",
    "            model[rep].hypergrad() # calculate gradient of hyperparameter ()\n",
    "            optimizer_super.step()\n",
    "            model[rep].reprojection()\n",
    "            \n",
    "        # update history \n",
    "        lambda_hist[:,epoch+1,rep] = model[rep].regularize.weight.data.flatten().cpu().numpy()\n",
    "        train_loss[epoch,rep] = running_train_loss/(i+1)\n",
    "        train_acc[epoch,rep] = running_train_acc/(i+1)\n",
    "        \n",
    "        # test\n",
    "        model[rep].eval()\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            x_test, y_test = test_data\n",
    "            x_test, y_test = x_test.to(mydevice), y_test.to(mydevice)\n",
    "            y_test_pred, hidden = model[rep](x_test)\n",
    "            loss = criterion_base(y_test_pred[-1,:,:],y_test)\n",
    "            # record performance on validation dataset\n",
    "            running_test_loss+=loss.item()\n",
    "            running_test_acc+=get_accuracy(y_test_pred[-1,:,:], y_test)\n",
    "            \n",
    "        # update history \n",
    "        test_loss[epoch,rep] = running_test_loss/(i+1)\n",
    "        test_acc[epoch,rep] = running_test_acc/(i+1)\n",
    "         \n",
    "    torch.save(model[rep].state_dict(), model_path)\n",
    "    if test_acc[-1,rep]>best_test_acc:\n",
    "        best_model = model[rep]\n",
    "        best_test_acc = test_acc[-1,rep]\n",
    "    model[rep] = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.mean(train0_loss,axis=1),'k-')\n",
    "plt.plot(np.mean(test0_loss,axis=1),'k--')\n",
    "plt.legend(['train (control)','test (control)','train (regularized)','test (regularized)'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.mean(train0_acc,axis=1),'k-')\n",
    "plt.plot(np.mean(test0_acc,axis=1),'k--')\n",
    "plt.legend(['train (control)','test (control)','train (regularized)','test (regularized)'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.mean(train_loss,axis=1),'b-')\n",
    "plt.plot(np.mean(test_loss,axis=1),'b--')\n",
    "plt.legend(['train (control)','test (control)','train (regularized)','test (regularized)'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.mean(train_acc,axis=1),'b-')\n",
    "plt.plot(np.mean(test_acc,axis=1),'b--')\n",
    "plt.legend(['train (control)','test (control)','train (regularized)','test (regularized)'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.mean(train0_loss,axis=1),np.mean(test0_loss,axis=1),'k-')\n",
    "plt.legend(['control','regularized'])\n",
    "plt.xlabel('train loss')\n",
    "plt.ylabel('test loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.mean(train0_acc,axis=1),np.mean(test0_acc,axis=1),'k-')\n",
    "plt.legend(['control','regularized'])\n",
    "plt.xlabel('train accuracy')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(np.mean(train_loss,axis=1),np.mean(test_loss,axis=1),'b-')\n",
    "plt.legend(['control','regularized'])\n",
    "plt.xlabel('train loss')\n",
    "plt.ylabel('test loss')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(np.mean(train_acc,axis=1),np.mean(test_acc,axis=1),'b-')\n",
    "plt.legend(['control','regularized'])\n",
    "plt.xlabel('train accuracy')\n",
    "plt.ylabel('test accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(np.mean(lambda_hist,axis=2)[:2,:].T)\n",
    "plt.ylim((0,0.2))\n",
    "plt.xlim((0,100))\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(np.mean(lambda_hist,axis=2)[14:,:].T)\n",
    "plt.ylim((0,0.2))\n",
    "plt.xlim((0,100))\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(np.mean(lambda_hist,axis=2)[2:6,:].T)\n",
    "plt.ylim((0,0.01))\n",
    "plt.xlim((0,100))\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot(np.mean(lambda_hist,axis=2)[6:10,:].T)\n",
    "plt.ylim((0,0.01))\n",
    "plt.xlim((0,100))\n",
    "plt.subplot(2,3,6)\n",
    "plt.plot(np.mean(lambda_hist,axis=2)[10:14,:].T)\n",
    "plt.ylim((0,0.01))\n",
    "plt.xlim((0,100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model0.eval()\n",
    "x, y_tar = next(iter(train_loader))\n",
    "y_pred, hidden = best_model0(x.to(mydevice))\n",
    "trial_samp = np.random.randint(x.shape[0])\n",
    "print(''.join(InvertAnna(torch.max(x[trial_samp,1:,:],1)[1]))+InvertAnna(y_tar)[trial_samp])\n",
    "print(''.join(InvertAnna(torch.max(y_pred[:,trial_samp,:].squeeze(),1)[1])))\n",
    "plt.plot(np.arange(1,11),y_pred.permute(1,2,0).data.cpu().numpy()[trial_samp,:,:].T,'-')\n",
    "plt.ylabel('Output')\n",
    "plt.xlabel('time step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.eval()\n",
    "x, y_tar = next(iter(train_loader))\n",
    "y_pred, hidden = best_model(x.to(mydevice))\n",
    "trial_samp = np.random.randint(x.shape[0])\n",
    "print(''.join(InvertAnna(torch.max(x[trial_samp,1:,:],1)[1]))+InvertAnna(y_tar)[trial_samp])\n",
    "print(''.join(InvertAnna(torch.max(y_pred[:,trial_samp,:].squeeze(),1)[1])))\n",
    "plt.plot(np.arange(1,11),y_pred.permute(1,2,0).data.cpu().numpy()[trial_samp,:,:].T,'-')\n",
    "plt.ylabel('Output')\n",
    "plt.xlabel('time step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized weight control\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "plt.subplot(1,8,1)\n",
    "plt.imshow(torch.cat((best_model0.encoder[0].weight.data,best_model0.encoder[1].weight.data)),cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,8,2)\n",
    "plt.imshow(best_model0.decoder.weight.data.cpu().numpy().T,cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(torch.cat((best_model0.recurrent[0].weight.data,best_model0.recurrent[1].weight.data)),cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(best_model0.modulator[0].weight.data,cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(best_model0.modulator[1].weight.data,cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.show()\n",
    "\n",
    "# regularized weight without sorting by input strength\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "plt.subplot(1,8,1)\n",
    "plt.imshow(torch.cat((best_model.encoder[0].weight.data,best_model.encoder[1].weight.data)),cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,8,2)\n",
    "plt.imshow(best_model.decoder.weight.data.cpu().numpy().T,cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(torch.cat((best_model.recurrent[0].weight.data,best_model.recurrent[1].weight.data)),cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(best_model.modulator[0].weight.data,cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(best_model.modulator[1].weight.data,cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shidx = torch.cat((best_model0.encoder[0].weight.data,best_model0.encoder[1].weight.data)).cpu().numpy().std(1).argsort()[::-1]\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "plt.title('Control')\n",
    "plt.subplot(1,8,1)\n",
    "plt.imshow(torch.cat((best_model0.encoder[0].weight.data,best_model0.encoder[1].weight.data)).cpu().numpy()[shidx,:],cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,8,2)\n",
    "plt.imshow(best_model0.decoder.weight.data.cpu().numpy()[:,shidx].T,cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(torch.cat((best_model0.recurrent[0].weight.data,best_model0.recurrent[1].weight.data)).cpu().numpy()[shidx,:][:,shidx],cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(best_model0.modulator[0].weight.data.cpu().numpy()[shidx,:][:,shidx],cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(best_model0.modulator[1].weight.data.cpu().numpy()[shidx,:,][:,shidx],cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "#fig.subplots_adjust(bottom=0.4, top=0.6)\n",
    "plt.show()\n",
    "\n",
    "shidx = torch.cat((best_model.encoder[0].weight.data,best_model.encoder[1].weight.data)).cpu().numpy().std(1).argsort()[::-1]\n",
    "fig = plt.figure(figsize=(20,4))\n",
    "plt.title('Block reguularirzed')\n",
    "plt.subplot(1,8,1)\n",
    "plt.imshow(torch.cat((best_model.encoder[0].weight.data,best_model.encoder[1].weight.data)).cpu().numpy()[shidx,:],cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,8,2)\n",
    "plt.imshow(best_model.decoder.weight.data.cpu().numpy()[:,shidx].T,cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(torch.cat((best_model.recurrent[0].weight.data,best_model.recurrent[1].weight.data)).cpu().numpy()[shidx,:][:,shidx],cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(best_model.modulator[0].weight.data.cpu().numpy()[shidx,:][:,shidx],cmap='seismic')\n",
    "plt.clim(-.25,.25)\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(best_model.modulator[1].weight.data.cpu().numpy()[shidx,:,][:,shidx],cmap='seismic')\n",
    "plt.clim(-.5,.5)\n",
    "#fig.subplots_adjust(bottom=0.4, top=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "#plt.subplot(1,3,1)\n",
    "plt.plot(torch.cat((best_model.encoder[0].weight.data,best_model.encoder[1].weight.data)).cpu().numpy().std(1)[shidx])\n",
    "plt.plot(torch.cat((best_model.recurrent[0].weight.data,best_model.recurrent[1].weight.data)).cpu().numpy().std(1)[shidx])\n",
    "plt.plot(best_model.modulator[1].weight.data.cpu().numpy().std(1)[shidx])\n",
    "#plt.subplot(1,3,2)\n",
    "#plt.plot(torch.cat((best_model.recurrent[0].weight.data,best_model.recurrent[1].weight.data)).cpu().numpy().mean(1))\n",
    "plt.plot(best_model.decoder.weight.data.cpu().numpy().std(0)[shidx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,4))\n",
    "plt.plot(torch.cat((best_model.encoder[0].bias.data,best_model.encoder[1].bias.data)).cpu().numpy()[shidx])\n",
    "plt.plot(torch.cat((best_model.recurrent[0].weight.data,best_model.recurrent[1].weight.data)).cpu().numpy().mean(1)[shidx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
